[Techniques Explanation](https://github.com/khchu93/ComputerVision/blob/main/notes/ComputerVisionTechniquesExplanation.md)

- Normalized initialization
- intermediate normalization layers

- Activation function
    - Sigmoid
        - Exploding/Vanishing gradient 
    - Tanh
        - Exploding/Vanishing gradient 
    - ReLU (Rectified Linear Unit)
        - Dying ReLU
