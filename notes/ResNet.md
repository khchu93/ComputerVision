# 

## Architecture
![alt text](https://github.com/khchu93/NoteImage/blob/main/resnet.jpg?raw=true) <br>



P.S. ResNet stands for Residual Network

## Key Achievements
- Enabled training of extremely deep neural networks (up to 152 layers at the time) by introducing Residual learning<sup>[1]</sup>, which fixed the vanishing/exploding gradients problem.
- Proof that the degradation problems of previous deep neural networks are not caused by overfitting or by vanishing/exploding gradients, but by optimization difficulty.
    - 

<sup>[1]</sup> Residual learning (= skip connections): 
## Pros & Cons

Pros
- 

Cons
- 

## When to use

## Implementation
- Framework: 
- Dataset: 
- Colab Notebook: [link]()

<!--
## Results
Training

Validation

Examples:
-->

## References
[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)

