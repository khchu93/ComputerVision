
# Faster R-CNN

## Motivation

Recent object detection systems, which rely on region proposal algorithms and R-CNN variants like Fast R-CNN, have successfully leveraged deep convolutional networks to drastically reduce the running time of the detection phase itself. When ignoring the time spent generating proposals, Fast R-CNN can achieve near real-time speeds. However, this improvement exposes the **region proposal step as the test-time computational bottleneck**. Traditional methods used for generating proposals, such as **`Selective Search (SS)`**, are **extremely slow**, taking about **2 seconds per image** when implemented on a CPU. Even state-of-the-art proposal methods like `EdgeBoxes` **consume as much running time as the detection network itself**, hindering overall system speed.

The authors’ motivation is to **eliminate this bottleneck** by introducing an algorithmic change: **using a deep convolutional neural network (CNN) for proposal generation**. They propose the **`Region Proposal Network (RPN)`**, which is designed to **share convolutional layers with the existing detection network (`Fast R-CNN`)**. By **sharing** these full-image **convolutional features**, the marginal **cost for computing proposals becomes small**—for example, just **10 milliseconds per image** at test-time. This allows the RPN and Fast R-CNN to be merged into a single, unified network, enabling nearly cost-free region proposals and achieving real-time performance.

## Architecture
![alt text](https://github.com/khchu93/NoteImage/blob/main/fasterRCNN.jpg?raw=true) <br>
The `Faster R-CNN` object detection system is constructed as a single, unified network composed of **two primary modules**: the **`Region Proposal Network (RPN)`** and the **`Fast R-CNN` detector**. `The crucial computational optimization in this architecture is that both the RPN and the detection network share a common set of convolutional layers`. This feature sharing means the **marginal cost of computing region proposals becomes very small** (e.g., 10ms per image), **eliminating the previous computational bottleneck** associated with external proposal methods.

1. **The `Region Proposal Network (RPN)`**<br>

The `RPN` is the first module and **acts as a `fully convolutional network (FCN)`** designed to **generate high-quality object proposals**.<br>
- **Shared Convolutional Features**: The process begins with the input image **passing through a series of shared convolutional layers** (e.g., 5 layers for ZF net or 13 layers for VGG-16) to produce a **convolutional feature map**.<br>
- **Sliding Window and Mini-Network**: To generate proposals, a **small network** (called a mini-network) **slides over this feature map**. At each sliding-window location, this spatial window is **mapped to a lower-dimensional feature** (e.g., 512-d for VGG), which is then fed into **two sibling 1×1 convolutional layers**: the **box-classification layer (cls)** and the **box-regression layer (reg)**. Since this mini-network operates in a sliding-window fashion, its **parameters are shared** across all spatial locations.
- **Anchors for Multi-Scale Prediction**: The `RPN` is designed to efficiently predict regions across a wide range of scales and aspect ratios using a novel concept called **anchors**<sup>[1]</sup>. At each sliding position, the `RPN` simultaneously **predicts multiple proposals** (denoted as k) relative to these reference anchors.
  - The **cls layer** outputs **2k scores** estimating the probability of an object versus background for each anchor.
  - The **reg layer** outputs **4k coordinates** encoding the bounding box adjustments relative to the anchors.<br>
  
By default, the system uses **k=9 anchors** at each location, generated by combining **3 scales** (e.g., $128^2$ , $256^2$ , $512^2$ pixels) and **3 aspect ratios** (1:1, 1:2, 2:1). This strategy allows the RPN to **handle multi-scale prediction** using a single-scale input image and fixed-size filters, effectively serving as a "pyramid of regression references".

2. **The `Fast R-CNN` Detector Module**<br>

The proposals generated by the `RPN` are then fed into the second module, the `Fast R-CNN` detector. Using the terminology of **attention mechanisms**, the `RPN serves to tell the unified network "where to look"`.

- **Detection Process**: The Fast R-CNN detector uses the **shared convolutional features** and the **`RPN`'s proposals** to perform **two main tasks**: **final object classification** (into specific categories) and **final bounding box refinement** (regression). The `RPN` proposals are processed through the **RoI pooling layer** and **subsequent detection layers** to yield the final detection results.

<sup>[1]</sup> Anchor: A predefined reference box at a given location on the feature map, based on scale and aspect ratio: (x, y, w, h) - center, width, height.

## Key Achievements
- Introduce the Region Proposal Network (RPN) to replace the external region proposal method
- Share a common set of full-image convolutional features between RPN and fast R-CNN module to reduce the expensive initial feature computation to run only once time
- Introduce anchors at each spatial location to handle multiple scales and aspect ratios efficiently

## Pros & Cons

Pros
- The model is now end-to-end trainable, unlike the previous R-CNN and Fast R-CNN, which relied on an external region proposal method
- Higher quality proposals with the learnable RPN that proposes and adjusts at the same time the localization of the proposals
- Much faster by replacing the slow external region proposal method (CPU-bound, ~2s per image) with a module that can utilize GPU during the forward pass (~0.2s per image)

Cons
- Slower Inference Speed: Being a two-stage detector, it’s significantly slower than one-stage models (YOLO, SSD, RetinaNet), especially on real-time applications.
- High Computational Cost: The RPN and detection head both process thousands of anchors, leading to heavy memory usage and longer training/inference time.
- Anchor-Dependent: Performance relies on good anchor design (scales/aspect ratios). Poorly matched anchors can degrade detection accuracy.
- Struggle with highly overlapping or crowded scenes: In dense scenes (e.g., pedestrians in a crowd), NMS often suppresses true positives that have high IoU overlap, keeping only one detection.

<!--
## When to use

## Implementation
- Framework: 
- Dataset: 
- Colab Notebook: [link]()


## Results
Training

Validation

Examples:
-->

## References
[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497) <br>
[Faster R-CNN: Object Detection](https://medium.com/thedeephub/faster-r-cnn-object-detection-5dfe77104e31)
[Faster R-CNN (Object detection)](https://www.linkedin.com/pulse/faster-r-cnn-object-detection-jizong-zhan-rr5ge/)
